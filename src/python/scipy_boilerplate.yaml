# CoCalc Examples Documentation File
# Copyright: CoCalc Authors, 2018
# This is derived content from the BSD licensed https://github.com/moble/jupyter_boilerplate/

# # # # # # # # # # # # # # # # # # # # # # # # # # #
# THIS FILE IS AUTOGENERATED -- DO NOT EDIT BY HAND #
# # # # # # # # # # # # # # # # # # # # # # # # # # #

---
language: python
---
category: ['SciPy', 'Fast Fourier Transform routines']
setup: "import numpy as np\nimport scipy\nfrom scipy import integrate, optimize, interpolate"
---
title: |
  Setup
code: |
  from scipy import fftpack
---
category: ['SciPy', 'Integration and ODE solvers']
setup: "import numpy as np\nimport scipy\nfrom scipy import integrate, optimize, interpolate"
---
title: |
  Setup
code: |
  from scipy import integrate
---
category: ['SciPy', 'Integration and ODE solvers / Integrate given function object']
setup: "import numpy as np\nimport scipy\nfrom scipy import integrate, optimize, interpolate"
---
title: |
  General-purpose integration
code: |
  from scipy import integrate
  def f(x, a, b):
      return a * x + b
  integral,error = integrate.quad(f, 0, 4.5, args=(2,1))  # integrates 2*x+1
  print(integral, error)
---
title: |
  General purpose double integration
code: |
  from scipy import integrate
  def integrand(y, x):
      return x * y**2
  x_lower_lim, x_upper_lim = 0.0, 0.5
  y_lower_lim, y_upper_lim = lambda x:0.0, lambda x:1.0-2.0*x
  # int_{x=0}^{0.5} int_{y=0}^{1-2x} x y dx dy
  integral,error = integrate.dblquad(integrand,
                                     x_lower_lim, x_upper_lim,
                                     y_lower_lim, y_upper_lim)
  print(integral, error)
---
title: |
  General purpose triple integration
code: |
  from scipy import integrate
  def integrand(z, y, x):
      return x * y**2 + z
  x_lower_lim, x_upper_lim = 0.0, 0.5
  y_lower_lim, y_upper_lim = lambda x:0.0, lambda x:1.0-2.0*x
  z_lower_lim, z_upper_lim = lambda x,y:-1.0, lambda x,y:1.0+2.0*x-y
  # int_{x=0}^{0.5} int_{y=0}^{1-2x} int_{z=-1}^{1+2x-y} (x y**2 + z) dz dy dx
  integral,error = integrate.tplquad(integrand,
                                     x_lower_lim, x_upper_lim,
                                     y_lower_lim, y_upper_lim,
                                     z_lower_lim, z_upper_lim)
  print(integral, error)
---
title: |
  General purpose n-fold integration
code: |
  from scipy import integrate
  def integrand(x0, x1, x2):
      return x2 * x1**2 + x0
  x2_lim = (0.0, 0.5)
  x1_lim = lambda x2:(0.0, 1.0-2.0*x2)
  x0_lim = lambda x1,x2:(-1.0, 1.0+2.0*x2-x1)
  # int_{x2=0}^{0.5} int_{x1=0}^{1-2x2} int_{x0=-1}^{1+2x2-x1} (x2 x1**2 + x0) dx0 dx1 dx2
  integral,error = integrate.nquad(integrand, [x0_lim, x1_lim, x2_lim])
  print(integral, error)
---
title: |
  Integrate func(x) using Gaussian quadrature of order $n$
code: |
  gaussian = lambda x: 1/np.sqrt(np.pi) * np.exp(-x**2)
  a,b = 0,1  # limits of integration
  result,err = integrate.fixed_quad(gaussian, a, b, n=5)
---
title: |
  Integrate with given tolerance using Gaussian quadrature
code: |
  gaussian = lambda x: 1/np.sqrt(np.pi) * np.exp(-x**2)
  a,b = 0,1  # limits of integration
  result,err = integrate.quadrature(gaussian, a, b, tol=1e-8, rtol=1e-8)
---
title: |
  Integrate using Romberg integration
code: |
  gaussian = lambda x: 1/np.sqrt(np.pi) * np.exp(-x**2)
  a,b = 0,1  # limits of integration
  result = integrate.romberg(gaussian, a, b, tol=1e-8, rtol=1e-8)
---
category: ['SciPy', 'Integration and ODE solvers / Integrate given fixed samples']
setup: "import numpy as np\nimport scipy\nfrom scipy import integrate, optimize, interpolate"
---
title: |
  Trapezoidal rule to compute integral from samples
code: |
  x = np.linspace(1, 5, num=100)
  y = 3*x**2 + 1
  integrate.trapz(y, x) # Exact value is 128
---
title: |
  Trapezoidal rule to cumulatively compute integral from samples
code: |
  x = np.linspace(1, 5, num=100)
  y = 3*x**2 + 1
  integrate.cumtrapz(y, x) # Should range from ~0 to ~128
---
title: |
  Simpson's rule to compute integral from samples
code: |
  x = np.linspace(1, 5, num=100)
  y = 3*x**2 + 1
  integrate.simps(y, x) # Exact value is 128
---
title: |
  Romberg Integration to compute integral from $2^k + 1$ evenly spaced samples
code: |
  x = np.linspace(1, 5, num=2**7+1)
  y = 3*x**2 + 1
  integrate.romb(y, x) # Exact value is 128
---
category: ['SciPy', 'Integration and ODE solvers / Numerically integrate ODE systems']
setup: "import numpy as np\nimport scipy\nfrom scipy import integrate, optimize, interpolate"
---
title: |
  General integration of ordinary differential equations
code: |
  from scipy.special import gamma, airy
  def func(y, t):
      return [t*y[1], y[0]]
  x = np.arange(0, 4.0, 0.01)
  y_0 = [-1.0 / 3**(1.0/3.0) / gamma(1.0/3.0), 1.0 / 3**(2.0/3.0) / gamma(2.0/3.0)]
  Ai, Aip, Bi, Bip = airy(x)
  y = integrate.odeint(func, y_0, x, rtol=1e-12, atol=1e-12) # Exact answer: (Aip, Ai)
---
title: |
  General integration of ordinary differential equations with known gradient
code: |
  from scipy.special import gamma, airy
  def func(y, t):
      return [t*y[1], y[0]]
  def gradient(y, t):
      return [[0,t], [1,0]]
  x = np.arange(0, 4.0, 0.01)
  y_0 = [-1.0 / 3**(1.0/3.0) / gamma(1.0/3.0), 1.0 / 3**(2.0/3.0) / gamma(2.0/3.0)]
  Ai, Aip, Bi, Bip = airy(x)
  y = integrate.odeint(func, y_0, x, rtol=1e-12, atol=1e-12, Dfun=gradient) # Exact answer: (Aip, Ai)
---
title: |
  Integrate ODE using VODE and ZVODE routines
code: |
  def f(t, y, arg1):
      return [1j*arg1*y[0] + y[1], -arg1*y[1]**2]
  def jac(t, y, arg1):
      return [[1j*arg1, 1], [0, -arg1*2*y[1]]]
  y0 = [1.0j, 2.0]
  t0, t1, dt = 0.0, 10.0, 1.0
  r = integrate.ode(f, jac).set_integrator('zvode', method='bdf')
  r.set_initial_value(y0, t0)
  r.set_f_params(2.0)
  r.set_jac_params(2.0)
  while r.successful() and r.t < t1:
      r.integrate(r.t+dt)
      print('{0}: {1}'.format(r.t, r.y))
---
category: ['SciPy', 'Interpolation']
setup: "import numpy as np\nimport scipy\nfrom scipy import integrate, optimize, interpolate"
---
title: |
  Setup
code: |
  from scipy import interpolate
---
title: |
  interp1d
code: |
  # NOTE: `interp1d` is very slow; prefer `InterpolatedUnivariateSpline`
  x = np.linspace(0, 10, 10)
  y = np.cos(-x**2/8.0)
  f = interpolate.interp1d(x, y, kind='cubic')
  X = np.linspace(0, 10, 100)
  Y = f(X)
---
title: |
  splrep / splrev
code: |
  x = np.arange(0, 2*np.pi+np.pi/4, 2*np.pi/8)
  y = np.sin(x)
  tck = interpolate.splrep(x, y, s=0)
  xnew = np.arange(0,2*np.pi,np.pi/50)
  ynew = interpolate.splev(xnew, tck, der=0)
---
title: |
  InterpolatedUnivariateSpline
code: |
  x = np.arange(0, 2*np.pi+np.pi/4, 2*np.pi/8)
  y = np.sin(x)
  s = interpolate.InterpolatedUnivariateSpline(x, y)
  xnew = np.arange(0, 2*np.pi, np.pi/50)
  ynew = s(xnew)
---
category: ['SciPy', 'Interpolation / Multivariate interpolation']
setup: "import numpy as np\nimport scipy\nfrom scipy import integrate, optimize, interpolate"
---
category: ['SciPy', 'Interpolation / 2-D Splines']
setup: "import numpy as np\nimport scipy\nfrom scipy import integrate, optimize, interpolate"
---
category: ['SciPy', 'Interpolation / Radial basis functions']
setup: "import numpy as np\nimport scipy\nfrom scipy import integrate, optimize, interpolate"
---
category: ['SciPy', 'Linear algebra']
setup: "import numpy as np\nimport scipy\nfrom scipy import integrate, optimize, interpolate"
---
title: |
  Setup
code: |
  from scipy import linalg
---
category: ['SciPy', 'Optimization']
setup: "import numpy as np\nimport scipy\nfrom scipy import integrate, optimize, interpolate"
---
title: |
  Setup
code: |
  from scipy import optimize
---
category: ['SciPy', 'Optimization / Scalar function minimization']
setup: "import numpy as np\nimport scipy\nfrom scipy import integrate, optimize, interpolate"
---
title: |
  Unconstrained minimization
code: |
  f = lambda x: (x - 2) * (x + 1)**2
  res = optimize.minimize_scalar(f, method='brent')
  print(res.x)
---
title: |
  Bounded minimization
code: |
  from scipy.special import j1  # Test function
  res = optimize.minimize_scalar(j1, bounds=(4, 7), method='bounded')
  print(res.x)
---
category: ['SciPy', 'Optimization / General-purpose optimization']
setup: "import numpy as np\nimport scipy\nfrom scipy import integrate, optimize, interpolate"
---
title: |
  Nelder-Mead Simplex algorithm
code: |
  def rosen(x):
      """The Rosenbrock function"""
      return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)
  x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])
  res = optimize.minimize(rosen, x0, method='nelder-mead',
                          options={'xatol': 1e-8, 'disp': True})
  print(res.x)
---
title: |
  Broyden-Fletcher-Goldfarb-Shanno (BFGS), analytical derivative
code: |
  def rosen(x):
      """The Rosenbrock function"""
      return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)
  def rosen_der(x):
      """Derivative of the Rosenbrock function"""
      xm = x[1:-1]
      xm_m1 = x[:-2]
      xm_p1 = x[2:]
      der = np.zeros_like(x)
      der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)
      der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])
      der[-1] = 200*(x[-1]-x[-2]**2)
      return der
  x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])
  res = optimize.minimize(rosen, x0, method='BFGS', jac=rosen_der, options={'disp': True})
  print(res.x)
---
title: |
  Broyden-Fletcher-Goldfarb-Shanno (BFGS), finite-difference derivative
code: |
  def rosen(x):
      """The Rosenbrock function"""
      return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)
  x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])
  res = optimize.minimize(rosen, x0, method='BFGS', options={'disp': True})
  print(res.x)
---
title: |
  Newton-Conjugate-Gradient, full Hessian
code: |
  def rosen(x):
      """The Rosenbrock function"""
      return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)
  def rosen_der(x):
      """Derivative of the Rosenbrock function"""
      xm = x[1:-1]
      xm_m1 = x[:-2]
      xm_p1 = x[2:]
      der = np.zeros_like(x)
      der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)
      der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])
      der[-1] = 200*(x[-1]-x[-2]**2)
      return der
  def rosen_hess(x):
      x = np.asarray(x)
      H = np.diag(-400*x[:-1],1) - np.diag(400*x[:-1],-1)
      diagonal = np.zeros_like(x)
      diagonal[0] = 1200*x[0]-400*x[1]+2
      diagonal[-1] = 200
      diagonal[1:-1] = 202 + 1200*x[1:-1]**2 - 400*x[2:]
      H = H + np.diag(diagonal)
      return H
  x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])
  res = optimize.minimize(rosen, x0, method='Newton-CG', jac=rosen_der, hess=rosen_hess,
                          options={'xtol': 1e-8, 'disp': True})
  print(res.x)
---
title: |
  Newton-Conjugate-Gradient, Hessian product
code: |
  def rosen(x):
      """The Rosenbrock function"""
      return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)
  def rosen_der(x):
      """Derivative of the Rosenbrock function"""
      xm = x[1:-1]
      xm_m1 = x[:-2]
      xm_p1 = x[2:]
      der = np.zeros_like(x)
      der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)
      der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])
      der[-1] = 200*(x[-1]-x[-2]**2)
      return der
  def rosen_hess_p(x,p):
      x = np.asarray(x)
      Hp = np.zeros_like(x)
      Hp[0] = (1200*x[0]**2 - 400*x[1] + 2)*p[0] - 400*x[0]*p[1]
      Hp[1:-1] = (-400*x[:-2]*p[:-2]+(202+1200*x[1:-1]**2-400*x[2:])*p[1:-1] 
                  -400*x[1:-1]*p[2:])
      Hp[-1] = -400*x[-2]*p[-2] + 200*p[-1]
      return Hp
  x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])
  res = optimize.minimize(rosen, x0, method='Newton-CG', jac=rosen_der, hessp=rosen_hess_p,
                          options={'xtol': 1e-8, 'disp': True})
  print(res.x)
---
category: ['SciPy', 'Optimization / Constrained multivariate minimization']
setup: "import numpy as np\nimport scipy\nfrom scipy import integrate, optimize, interpolate"
---
title: |
  Unconstrained Sequential Least SQuares Programming (SLSQP)
code: |
  def func(x, sign=1.0):
      """ Objective function """
      return sign*(2*x[0]*x[1] + 2*x[0] - x[0]**2 - 2*x[1]**2)
  def func_deriv(x, sign=1.0):
      """ Derivative of objective function """
      dfdx0 = sign*(-2*x[0] + 2*x[1] + 2)
      dfdx1 = sign*(2*x[0] - 4*x[1])
      return np.array([ dfdx0, dfdx1 ])
  res = optimize.minimize(func, [-1.0,1.0], args=(-1.0,), jac=func_deriv,
                          method='SLSQP', options={'disp': True})
  print(res.x)
---
title: |
  Constrained Sequential Least SQuares Programming (SLSQP)
code: |
  def func(x, sign=1.0):
      """ Objective function """
      return sign*(2*x[0]*x[1] + 2*x[0] - x[0]**2 - 2*x[1]**2)
  def func_deriv(x, sign=1.0):
      """ Derivative of objective function """
      dfdx0 = sign*(-2*x[0] + 2*x[1] + 2)
      dfdx1 = sign*(2*x[0] - 4*x[1])
      return np.array([ dfdx0, dfdx1 ])
  # Constraints correspond to x**3-y=0 and y-1>=0, respectively
  cons = ({'type': 'eq',
           'fun' : lambda x: np.array([x[0]**3 - x[1]]),
           'jac' : lambda x: np.array([3.0*(x[0]**2.0), -1.0])},
          {'type': 'ineq',
           'fun' : lambda x: np.array([x[1] - 1]),
           'jac' : lambda x: np.array([0.0, 1.0])})
  res = optimize.minimize(func, [-1.0,1.0], args=(-1.0,), jac=func_deriv,
                          constraints=cons, method='SLSQP', options={'disp': True})
  print(res.x)
---
category: ['SciPy', 'Optimization / Fitting (see also numpy.polynomial)']
setup: "import numpy as np\nimport scipy\nfrom scipy import integrate, optimize, interpolate"
---
title: |
  Basic function fitting
code: |
  def fitting_function(x, a, b, c):
      return a * np.exp(-b * x) + c
  xdata = np.linspace(0, 4, 50)
  ydata = fitting_function(xdata, 2.5, 1.3, 0.5) + 0.2 * np.random.normal(size=len(xdata))
  optimal_parameters, estimated_covariance = optimize.curve_fit(fitting_function, xdata, ydata)
  estimated_std_dev = np.sqrt(np.diag(estimated_covariance))
---
category: ['SciPy', 'Statistical distributions and functions']
setup: "import numpy as np\nimport scipy\nfrom scipy import integrate, optimize, interpolate"
---
title: |
  Setup
code: |
  from scipy import stats